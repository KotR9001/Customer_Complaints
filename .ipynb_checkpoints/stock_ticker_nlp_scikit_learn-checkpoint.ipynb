{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Latest Version of Spark\n",
    "#%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3 \n",
    "\n",
    " \n",
    "\n",
    "# \n",
    "\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more \n",
    "\n",
    "# contributor license agreements.  See the NOTICE file distributed with \n",
    "\n",
    "# this work for additional information regarding copyright ownership. \n",
    "\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0 \n",
    "\n",
    "# (the \"License\"); you may not use this file except in compliance with \n",
    "\n",
    "# the License.  You may obtain a copy of the License at \n",
    "\n",
    "# \n",
    "\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0 \n",
    "\n",
    "# \n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software \n",
    "\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, \n",
    "\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "# See the License for the specific language governing permissions and \n",
    "\n",
    "# limitations under the License. \n",
    "\n",
    "# \n",
    "\n",
    " \n",
    "\n",
    "# This script attempt to determine the correct setting for SPARK_HOME given \n",
    "\n",
    "# that Spark may have been installed on the system with pip. \n",
    "\n",
    " \n",
    "\n",
    "import os \n",
    "\n",
    "import sys \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "def _find_spark_home(): \n",
    "\n",
    "    \"\"\"Find the SPARK_HOME.\"\"\" \n",
    "\n",
    "    # If the environment has SPARK_HOME set trust it. \n",
    "\n",
    "    if \"SPARK_HOME\" in os.environ: \n",
    "\n",
    "        return os.environ[\"SPARK_HOME\"] \n",
    "\n",
    " \n",
    "\n",
    "    def is_spark_home(path): \n",
    "\n",
    "        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\" \n",
    "\n",
    "        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and \n",
    "\n",
    "                (os.path.isdir(os.path.join(path, \"jars\")) or \n",
    "\n",
    "                 os.path.isdir(os.path.join(path, \"assembly\")))) \n",
    "\n",
    " \n",
    "\n",
    "    # Spark distribution can be downloaded when PYSPARK_HADOOP_VERSION environment variable is set. \n",
    "\n",
    "    # We should look up this directory first, see also SPARK-32017. \n",
    "\n",
    "    spark_dist_dir = \"spark-distribution\" \n",
    "\n",
    "    paths = [ \n",
    "\n",
    "        \"../\",  # When we're in spark/python. \n",
    "\n",
    "        # Two case belows are valid when the current script is called as a library. \n",
    "\n",
    "        os.path.join(os.path.dirname(os.path.realpath('spark-3.3.2-bin-hadoop3.tgz')), spark_dist_dir), \n",
    "\n",
    "        os.path.dirname(os.path.realpath('spark-3.3.2-bin-hadoop3.tgz'))] \n",
    "\n",
    " \n",
    "\n",
    "    # Add the path of the PySpark module if it exists \n",
    "\n",
    "    import_error_raised = False \n",
    "\n",
    "    from importlib.util import find_spec \n",
    "\n",
    "    try: \n",
    "\n",
    "        module_home = os.path.dirname(find_spec(\"pyspark\").origin) \n",
    "\n",
    "        paths.append(os.path.join(module_home, spark_dist_dir)) \n",
    "\n",
    "        paths.append(module_home) \n",
    "\n",
    "        # If we are installed in edit mode also look two dirs up \n",
    "\n",
    "        # Downloading different versions are not supported in edit mode. \n",
    "\n",
    "        paths.append(os.path.join(module_home, \"../../\")) \n",
    "\n",
    "    except ImportError: \n",
    "\n",
    "        # Not pip installed no worries \n",
    "\n",
    "        import_error_raised = True \n",
    "\n",
    " \n",
    "\n",
    "    # Normalize the paths \n",
    "\n",
    "    paths = [os.path.abspath(p) for p in paths] \n",
    "\n",
    " \n",
    "\n",
    "    try: \n",
    "\n",
    "        return next(path for path in paths if is_spark_home(path)) \n",
    "\n",
    "    except StopIteration: \n",
    "\n",
    "        print(\"Could not find valid SPARK_HOME while searching {0}\".format(paths), file=sys.stderr) \n",
    "\n",
    "        if import_error_raised: \n",
    "\n",
    "            print( \n",
    "\n",
    "                \"\\nDid you install PySpark via a package manager such as pip or Conda? If so,\\n\" \n",
    "\n",
    "                \"PySpark was not found in your Python environment. It is possible your\\n\" \n",
    "\n",
    "                \"Python environment does not properly bind with your package manager.\\n\" \n",
    "\n",
    "                \"\\nPlease check your default 'python' and if you set PYSPARK_PYTHON and/or\\n\" \n",
    "\n",
    "                \"PYSPARK_DRIVER_PYTHON environment variables, and see if you can import\\n\" \n",
    "\n",
    "                \"PySpark, for example, 'python -c 'import pyspark'.\\n\" \n",
    "\n",
    "                \"\\nIf you cannot import, you can install by using the Python executable directly,\\n\" \n",
    "\n",
    "                \"for example, 'python -m pip install pyspark [--user]'. Otherwise, you can also\\n\" \n",
    "\n",
    "                \"explicitly set the Python executable, that has PySpark installed, to\\n\" \n",
    "\n",
    "                \"PYSPARK_PYTHON or PYSPARK_DRIVER_PYTHON environment variables, for example,\\n\" \n",
    "\n",
    "                \"'PYSPARK_PYTHON=python3 pyspark'.\\n\", file=sys.stderr) \n",
    "\n",
    "        sys.exit(-1) \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "    print(_find_spark_home()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge scitime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import wget\n",
    "\n",
    "# Install Java, Spark, and Findspark\n",
    "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#!python -m wget https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "#!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "#!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-20\"\n",
    "#os.environ[\"SPARK_HOME\"] = \"C:/Users/bjros/Anaconda3/Lib/site-packages/pyspark\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init(r\"C:\\Users\\bjros\\Anaconda3\\Lib\\site-packages\\pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found method to increase Heap Size at https://stackoverflow.com/questions/32336915/pyspark-java-lang-outofmemoryerror-java-heap-space\n",
    "spark = SparkSession.builder.appName(\"NaiveBayes\").config(\"spark.driver.memory\", \"15g\").config(\"spark.driver.maxResultSize\", \"0\").config(\"spark.executor.heartbeatInterval\", \"200000\").config(\"spark.network.timeout\", \"300000\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found Method to Speed Up Converting Python Objects to DataFrames at spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Data from the CSV Files\n",
    "#train_df = pd.read_csv('train.csv')\n",
    "#train_df['description_x_length'] = train_df['description_x'].str.len()\n",
    "#train_df['description_y_length'] = train_df['description_y'].str.len()\n",
    "\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "train_df = spark.read.csv('train.csv', sep=\",\", header=True)\n",
    "train_df = train_df.withColumn('description_x_length', length(train_df['description_x']))\n",
    "train_df = train_df.withColumn('description_y_length', length(train_df['description_y']))\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pd.read_csv('test.csv')\n",
    "#test_df['description_x_length'] = test_df['description_x'].str.len()\n",
    "#test_df['description_y_length'] = test_df['description_y'].str.len()\n",
    "\n",
    "test_df = spark.read.csv('test.csv', sep=\",\", header=True)\n",
    "test_df = test_df.withColumn('description_x_length', length(test_df['description_x']))\n",
    "test_df = test_df.withColumn('description_y_length', length(test_df['description_y']))\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tokenize the Description Columns in the DataFrames\n",
    "train_df['description_x_list'] = train_df['description_x'].apply(lambda x : word_tokenize(x))\n",
    "train_df['description_x_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df['description_y_list'] = train_df['description_y'].apply(lambda x : word_tokenize(x))\n",
    "train_df['description_y_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_df['description_x_list'] = test_df['description_x'].apply(lambda x : word_tokenize(x))\n",
    "test_df['description_x_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_df['description_y_list'] = test_df['description_y'].apply(lambda x : word_tokenize(x))\n",
    "test_df['description_y_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram, HashingTF, IDF, StringIndexer\n",
    "# Create all the features to the data set\n",
    "ticker_X = StringIndexer(inputCol='ticker_x',outputCol='label')\n",
    "ticker_Y = StringIndexer(inputCol='ticker_y',outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_X = Tokenizer(inputCol=\"description_x\", outputCol=\"description_x_tokenized\")\n",
    "tokenizer_Y = Tokenizer(inputCol=\"description_y\", outputCol=\"description_y_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopremove_X = StopWordsRemover(inputCol='description_x_tokenized',outputCol='description_x_tokenized_filtered')\n",
    "stopremove_Y = StopWordsRemover(inputCol='description_y_tokenized',outputCol='description_y_tokenized_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Different NGram Values\n",
    "ngram_X = NGram(n=3, inputCol='description_x_tokenized_filtered',outputCol='description_x_ngramed')\n",
    "ngram_Y = NGram(n=3, inputCol='description_y_tokenized_filtered',outputCol='description_y_ngramed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF_X = HashingTF(inputCol='description_x_ngramed', outputCol='description_x_hashed')\n",
    "hashingTF_Y = HashingTF(inputCol='description_y_ngramed', outputCol='description_y_hashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_X = IDF(inputCol='description_x_hashed', outputCol='description_x_idf')\n",
    "idf_Y = IDF(inputCol='description_y_hashed', outputCol='description_y_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up_X = VectorAssembler(inputCols=['description_x_idf', 'description_x_length'], outputCol='features')\n",
    "clean_up_Y = VectorAssembler(inputCols=['description_y_idf', 'description_y_length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline_X = Pipeline(stages=[ticker_X, tokenizer_X, stopremove_X, ngram_X, hashingTF_X, idf_X, clean_up_X])\n",
    "data_prep_pipeline_Y = Pipeline(stages=[ticker_Y, tokenizer_Y, stopremove_Y, ngram_Y, hashingTF_Y, idf_Y, clean_up_Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Convert Pandas DataFrames into Spark DataFrames\n",
    "#Credit to https://stackoverflow.com/questions/55604506/error-attributeerror-dataframe-object-has-no-attribute-jdf\n",
    "#from pyspark.sql import SQLContext\n",
    "\n",
    "#sqlContext = SQLContext(spark)\n",
    "\n",
    "spark_train_df = sqlContext.createDataFrame(train_df)\n",
    "spark_test_df = sqlContext.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#spark_train_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#spark_test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner_X = data_prep_pipeline_X.fit(train_df)\n",
    "cleaner_Y = data_prep_pipeline_Y.fit(train_df)\n",
    "\n",
    "cleaned_X = cleaner_X.transform(train_df)\n",
    "cleaned_Y = cleaner_Y.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show label and resulting features\n",
    "cleaned_X.select(['label', 'features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_Y.select(['label', 'features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark[sql]\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Try a Naive Bayes Model for Classification\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "train_df_train_X, train_df_test_X = cleaned_X.randomSplit([0.7, 0.3])\n",
    "train_df_train_Y, train_df_test_Y = cleaned_Y.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor_X = nb.fit(train_df_train_X)\n",
    "predictor_Y = nb.fit(train_df_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert PySpark DataFrame(s) to Pandas DataFrame(s)\n",
    "cleaned_X_Pandas = cleaned_X.toPandas()\n",
    "cleaned_Y_Pandas = cleaned_Y.toPandas()\n",
    "\n",
    "#Method Found at https://sparkbyexamples.com/pyspark/pyspark-collect/\n",
    "#cleaned_X_Pandas = pd.DataFrame.from_records(cleaned_X.collect(), columns=cleaned_X.columns)\n",
    "#cleaned_Y_Pandas = pd.DataFrame.from_records(cleaned_Y.collect(), columns=cleaned_Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Try a Logistic Regression Model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Break data down into a training set and a testing set\n",
    "train_df_train_X, train_df_test_X = cleaned_X.randomSplit([0.7, 0.3])\n",
    "train_df_train_Y, train_df_test_Y = cleaned_Y.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Logistic Regression model and fit training data\n",
    "lr = LogisticRegression()\n",
    "predictor_X = lr.fit(train_df_train_X)\n",
    "predictor_Y = lr.fit(train_df_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# dividing X, y into train and test data\n",
    "train_df_train_X_features, train_df_test_X_features, train_df_train_X_labels, train_df_test_X_labels = train_test_split(cleaned_X_Pandas['features'], cleaned_X_Pandas['label'], random_state = 42)\n",
    "train_df_train_Y_features, train_df_test_Y_features, train_df_train_Y_labels, train_df_test_Y_labels = train_test_split(cleaned_Y_Pandas['features'], cleaned_Y_Pandas['label'], random_state = 42)                                                                                                                                      \n",
    "  \n",
    "\n",
    "  \n",
    "# creating a confusion matrix\n",
    "#cm_X = confusion_matrix(train_df_test_X_labels, dtree_predictions_X)\n",
    "#cm_Y = confusion_matrix(train_df_test_Y_labels, dtree_predictions_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Models\n",
    "dtree_model_X = DecisionTreeClassifier(max_depth = 2)\n",
    "dtree_model_Y = DecisionTreeClassifier(max_depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --install-option='--prefix=C:\\Users\\bjros\\Anaconda3\\Lib\\site-packages' scitime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge scitime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to See how long the Model Would Take to Fit\n",
    "import time\n",
    "\n",
    "from scitime import RuntimeEstimator\n",
    "estimator = RuntimeEstimator(meta_algo='RF', verbose=3)\n",
    "\n",
    "estimation_X, lower_bound_X, upper_bound_X = estimator.time(dtree_model_X, list(train_df_train_X_features))\n",
    "estimation_Y, lower_bound_Y, upper_bound_Y = estimator.time(dtree_model_Y, list(train_df_train_Y_features))\n",
    "print(f\"The time estimation for fitting dtree_model_X is: {estimation_X}. The lower bound is: {lower_bound_X}. The upper bound is: {upper_bound_X}.\")\n",
    "print(f\"The time estimation for fitting dtree_model_Y is: {estimation_Y}. The lower bound is: {lower_bound_Y}. The upper bound is: {upper_bound_Y}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a DescisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model_Y_trained = dtree_model_X.fit(list(train_df_train_X_features), train_df_train_X_labels)\n",
    "dtree_model_Y_trained = dtree_model_Y.fit(list(train_df_train_Y_features), train_df_train_Y_labels)\n",
    "\n",
    "dtree_predictions_X = dtree_model_X.predict(list(train_df_test_X_features))\n",
    "dtree_predictions_Y = dtree_model_Y.predict(list(train_df_test_Y_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Naive Bayes model with Bernoulli Model Type and fit training data\n",
    "nb_gaussian = NaiveBayes(modelType='gaussian')\n",
    "predictor_X_gaussian = nb_gaussian.fit(train_df_train_X)\n",
    "predictor_Y_gaussian = nb_gaussian.fit(train_df_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results_X = predictor_X.transform(train_df_test_X)\n",
    "test_results_X.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_results_Y = predictor_Y.transform(train_df_test_Y)\n",
    "test_results_Y.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform the Bernoulli Type model with the testing data\n",
    "test_results_X_gaussian = predictor_X_gaussian.transform(train_df_test_X)\n",
    "test_results_X_gaussian.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_results_Y_gaussian = predictor_Y_gaussian.transform(train_df_test_Y)\n",
    "test_results_Y_gaussian.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Test Accuracy of Base Model\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_X = acc_eval.evaluate(test_results_X)\n",
    "print(\"Accuracy of model at predicting Ticker_X values was: %f\" % acc_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_Y = acc_eval.evaluate(test_results_Y)\n",
    "print(\"Accuracy of model at predicting Ticker_Y values was: %f\" % acc_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy of Model with Gaussian Type\n",
    "acc_eval_gaussian = MulticlassClassificationEvaluator()\n",
    "acc_X_gaussian = acc_eval_gaussian.evaluate(test_results_X)\n",
    "print(\"Accuracy of Gaussian Type model at predicting Ticker_X values was: %f\" % acc_X_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc_eval_gaussian = MulticlassClassificationEvaluator()\n",
    "acc_Y_gaussian = acc_eval_gaussian.evaluate(test_results_Y)\n",
    "print(\"Accuracy of Gaussian Type model at predicting Ticker_Y values was: %f\" % acc_Y_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Classification Report to Evaluate the Accuracy of the Decision Tree Model Predictions\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f'The results of the Decision Tree model in predicting tickers is: {classification_report(train_df_test_X_labels, dtree_predictions_X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The results of the Decision Tree model in predicting tickers is: {classification_report(train_df_test_Y_labels, dtree_predictions_Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://localhost:4040/api/v1/applications/[base-app-id]/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results\n",
    "#Multi-class Classification Evaluator\n",
    "#N Grams = 1\n",
    "#acc_X = 0.07\n",
    "#acc_Y = 0.11\n",
    "#acc_X_gaussian = 0.07\n",
    "#acc_Y_gaussian = 0.11\n",
    "\n",
    "#N Grams = 2\n",
    "#acc_X = 0.113362\n",
    "#acc_Y = 0.115141\n",
    "#acc_X_gaussian = 0.113362\n",
    "#acc_Y_gaussian = 0.115141\n",
    "\n",
    "#N Grams = 3\n",
    "#acc_X = 0.073060\n",
    "#acc_Y = 0.077853\n",
    "#acc_X_gaussian = 0.073060\n",
    "#acc_Y_gaussian = 0.077853\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "#acc_X =\n",
    "#acc_Y = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
